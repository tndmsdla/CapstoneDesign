#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ë‹¨ì¼ íŒŒì¼ AVSR inference ìŠ¤í¬ë¦½íŠ¸ (ìµœì¢…: ë¡œê·¸ ì‚­ì œ + í‚¤ ë§¤ì¹­ Fix + OOM ë°©ì§€)
"""

import sys
import os
import gc 
import torch
import torchaudio
import argparse
import logging
import psutil
import numpy as np
from python_speech_features import logfbank

# HuggingFace ê²½ê³  ë©”ì‹œì§€ ë„ê¸°
from transformers import logging as hf_logging
hf_logging.set_verbosity_error()

# ëª¨ë¸ ê´€ë ¨ ëª¨ë“ˆ ì„í¬íŠ¸
from models.lightning import ModelModule_LLM
from datamodule.av_dataset import load_video, cut_or_pad
from datamodule.data_module import collate_LLM
from datamodule.transforms import AudioTransform, VideoTransform

# ================= [ë©”ëª¨ë¦¬ í™•ì¸ í•¨ìˆ˜] =================
def print_memory_usage(step_name=""):
    """í˜„ì¬ CPU ë° GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
    # CPU RAM
    process = psutil.Process(os.getpid())
    cpu_mem = process.memory_info().rss / 1024**3  # GB ë‹¨ìœ„
    
    # GPU VRAM
    if torch.cuda.is_available():
        gpu_allocated = torch.cuda.memory_allocated() / 1024**3
        gpu_reserved = torch.cuda.memory_reserved() / 1024**3
        gpu_msg = f"GPU: {gpu_allocated:.2f}GB (Alloc) / {gpu_reserved:.2f}GB (Res)"
    else:
        gpu_msg = "GPU: N/A"
        
    print(f"[MEMORY] {step_name:<20} | CPU: {cpu_mem:.2f}GB | {gpu_msg}")
# =====================================================

def stacker(feats, stack_order):
    feat_dim = feats.shape[1]
    if len(feats) % stack_order != 0:
        res = stack_order - len(feats) % stack_order
        res = np.zeros([res, feat_dim]).astype(feats.dtype)
        feats = np.concatenate([feats, res], axis=0)
    feats = feats.reshape((-1, stack_order, feat_dim)).reshape(-1, stack_order*feat_dim)
    return feats

def parse_args():
    parser = argparse.ArgumentParser(description="AVSR ë‹¨ì¼ íŒŒì¼ inference")
    parser.add_argument("--checkpoint", type=str, required=True)
    parser.add_argument("--modality", type=str, required=True, choices=["audio", "video", "audiovisual", "audiovisual_avhubert"])
    parser.add_argument("--video_path", type=str, default=None)
    parser.add_argument("--audio_path", type=str, default=None)
    parser.add_argument("--output_txt", type=str, default="inference_result.txt")
    parser.add_argument("--pretrain-avhubert-enc-video-path", type=str, default=None)
    parser.add_argument("--pretrain-avhubert-enc-audio-path", type=str, default=None)
    parser.add_argument("--pretrain-avhubert-enc-audiovisual-path", type=str, default=None)
    parser.add_argument("--llm-model", type=str, default=None)
    parser.add_argument("--audio-encoder-name", type=str, default=None)
    parser.add_argument("--downsample-ratio-video", type=int, default=2)
    parser.add_argument("--downsample-ratio-audio", type=int, default=4)
    parser.add_argument("--max-dec-tokens", type=int, default=32)
    parser.add_argument("--num-beams", type=int, default=1)
    parser.add_argument("--use-lora-avhubert", action="store_true")
    parser.add_argument("--single-projector-avhubert", action="store_true")
    parser.add_argument("--grid-resample-audio", action="store_true")
    parser.add_argument("--use-uadf", action="store_true")
    parser.add_argument("--uadf-fusion-method", type=str, default="uncertainty")
    parser.add_argument("--uadf-temperature", type=float, default=1.0)
    parser.add_argument("--prompt-audio", type=str, default="Transcribe speech to text.")
    parser.add_argument("--prompt-video", type=str, default="Transcribe video to text.")
    parser.add_argument("--prompt-audiovisual", type=str, default="Transcribe speech and video to text.")
    parser.add_argument("--intermediate-size", type=int, default=2048)
    parser.add_argument("--unfrozen-modules", nargs="*", default=[None])
    parser.add_argument("--add_PETF_LLM", type=str, default=None)
    parser.add_argument("--reduction-lora", type=int, default=None)
    parser.add_argument("--alpha", type=int, default=None)
    parser.add_argument("--downsample-ratio-audiovisual", type=int, default=3)
    parser.add_argument("--pretrained-model-path", type=str, default=None)
    parser.add_argument("--use-half-precision", action="store_true")
    parser.add_argument("--low-cpu-mem-usage", action="store_true", default=True)
    parser.add_argument("--load-in-8bit", action="store_true")
    parser.add_argument("--cpu-offload", action="store_true")
    return parser.parse_args()

def load_model_from_checkpoint(checkpoint_path, args):
    """
    ë¡œê·¸ ìµœì†Œí™” + ìŠ¤ë§ˆíŠ¸ í‚¤ ë§¤ì¹­ + OOM ë°©ì§€ ë¡œë“œ
    """
    print("LLM & ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì‹œì‘")
    # print_memory_usage("Init Start")

    # 1. lightning.py ìë™ ë¡œë”© ë°©ì§€
    temp_path_backup = getattr(args, 'pretrained_model_path', None)
    args.pretrained_model_path = None 

    # 2. ì²´í¬í¬ì¸íŠ¸ í—¤ë” ì½ê¸° (ì„¤ì • ë™ê¸°í™”)
    try:
        ckpt = torch.load(checkpoint_path, map_location='cpu', mmap=True)
    except:
        ckpt = torch.load(checkpoint_path, map_location='cpu')

    # 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë™ê¸°í™”
    if 'hyper_parameters' in ckpt:
        hparams = ckpt['hyper_parameters']
        for key, value in hparams.items():
            if hasattr(args, key):
                setattr(args, key, value)

    # 4. í•„ìˆ˜ ê¸°ë³¸ê°’ ì„¤ì •
    defaults = {
        "audio_encoder_name": "openai/whisper-medium.en",
        "llm_model": "meta-llama/Meta-Llama-3.1-8B",
        "prompt_audio": "Transcribe speech to text.",
        "prompt_video": "Transcribe video to text.",
        "prompt_audiovisual": "Transcribe speech and video to text.",
        "downsample_ratio_audiovisual": 3,
        "intermediate_size": 2048,
        "modality": args.modality
    }
    for key, val in defaults.items():
        if not hasattr(args, key) or getattr(args, key) is None:
            setattr(args, key, val)

    # 5. ëª¨ë¸ ë¼ˆëŒ€ ìƒì„± (ì—¬ê¸°ì„œ ë¡œê·¸ê°€ ì¢€ ë‚˜ì˜¬ ìˆ˜ ìˆìŒ - ë¼ì´ë¸ŒëŸ¬ë¦¬ ìì²´ ë¡œê·¸)
    modelmodule = ModelModule_LLM(args)
    args.pretrained_model_path = temp_path_backup
    # print_memory_usage("Skeleton Built")

    # 6. ê°€ì¤‘ì¹˜ ë¡œë“œ (ìŠ¤ë§ˆíŠ¸ í‚¤ ë§¤ì¹­)
    if isinstance(ckpt, dict) and 'state_dict' in ckpt:
        state_dict = ckpt['state_dict']
    else:
        state_dict = ckpt
    
    # [Smart Key Matching] model. prefix ë¬¸ì œ ìë™ í•´ê²°
    model_keys = set(modelmodule.state_dict().keys())
    ckpt_keys = set(state_dict.keys())
    
    # 1) ê·¸ëŒ€ë¡œ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
    intersection = model_keys.intersection(ckpt_keys)
    
    # 2) ë§Œì•½ ë§¤ì¹­ë¥ ì´ ë‚®ìœ¼ë©´, prefix ìˆ˜ì • ì‹œë„
    if len(intersection) < len(model_keys) * 0.5:
        new_state_dict = {}
        for k, v in state_dict.items():
            # ckptì— model.ì´ ìˆê³ , ì½”ë“œì—” ì—†ìœ¼ë©´ ì œê±°
            if k.startswith("model.") and k[6:] in model_keys:
                new_state_dict[k[6:]] = v
            # ckptì— model.ì´ ì—†ê³ , ì½”ë“œì—” ìˆìœ¼ë©´ ì¶”ê°€
            elif "model." + k in model_keys:
                new_state_dict["model." + k] = v
            else:
                new_state_dict[k] = v
        state_dict = new_state_dict

    # ë¡œë“œ ì‹¤í–‰
    modelmodule.load_state_dict(state_dict, strict=False)
    
    del ckpt
    del state_dict
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # 7. í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¬ë™ê¸°í™” & UADF ì²˜ë¦¬
    if hasattr(modelmodule, "hparams"):
        for key, value in modelmodule.hparams.items():
            if not hasattr(args, key) or getattr(args, key) is None:
                setattr(args, key, value)
    if hasattr(args, 'use_uadf') and args.use_uadf and args.modality != "audiovisual":
        args.use_uadf = False

    # 8. GPU ì´ë™ ë° ì •ë°€ë„ ê°•ì œ í†µì¼ (BFloat16)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    modelmodule.eval()
    
    if device == "cuda":
        # ë³µì¡í•œ ë¡œê·¸ ì œê±°í•˜ê³  ì¡°ìš©íˆ ë³€í™˜
        target_modules = ["audio_encoder", "video_encoder", "audiovisual_encoder", "audio_proj", "video_proj", "audiovisual_proj", "uadf_block", "llm"]
        for mod_name in target_modules:
            if hasattr(modelmodule.model, mod_name):
                module = getattr(modelmodule.model, mod_name)
                if module is not None:
                    setattr(modelmodule.model, mod_name, module.to(device=device, dtype=torch.bfloat16))
        modelmodule.to(device=device, dtype=torch.bfloat16)

    # ë¡œì»¬ ê²½ë¡œ í† í¬ë‚˜ì´ì € ì´ë¦„ Fix
    modelmodule.tokenizer.name_or_path = "meta-llama/Meta-Llama-3.1-8B"

    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    print("LLM & ì²´í¬í¬ì¸íŠ¸ ë¡œë”© ì™„ë£Œ")
    # print_memory_usage("Ready to Infer")
    return modelmodule
def inference_single_file(args, modelmodule):
    """
    ë‹¨ì¼ íŒŒì¼ ì¶”ë¡  (ë¹„ë””ì˜¤ ë‹¨ì¼ ì…ë ¥ ì§€ì› + ì˜¤ë””ì˜¤/ë¹„ë””ì˜¤ ì°¨ì› êµì •)
    """
    print("ğŸš€ ì¶”ë¡  ì‹œì‘...")
    print_memory_usage("Before Inference")
    
    # Transform ì„¤ì •
    is_avhubert_audio = True if args.modality == 'audiovisual_avhubert' or (hasattr(args, 'audio_encoder_name') and args.audio_encoder_name == 'av-hubert' and args.modality == 'audio') else False
    video_transform = VideoTransform("test") if args.modality in ["video", "audiovisual", "audiovisual_avhubert"] else None
    audio_transform = AudioTransform("test", snr_target=999999, is_avhubert_audio=is_avhubert_audio) if args.modality in ["audio", "audiovisual", "audiovisual_avhubert"] else None
    
    batch_data = {}
    rate_ratio = 640
    
    # ================= [ìŠ¤ë§ˆíŠ¸ ê²½ë¡œ ì„¤ì • (í•µì‹¬ ìˆ˜ì •)] =================
    # ë¹„ë””ì˜¤ ê²½ë¡œë§Œ ìˆê³  ì˜¤ë””ì˜¤ ê²½ë¡œê°€ ì—†ìœ¼ë©´, ë¹„ë””ì˜¤ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì¶”ì¶œí•œë‹¤ê³  ê°€ì •
    if args.video_path and not args.audio_path:
        print(f"â„¹ï¸ [INFO] ì˜¤ë””ì˜¤ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. ë¹„ë””ì˜¤ íŒŒì¼({args.video_path})ì—ì„œ ì˜¤ë””ì˜¤ë¥¼ ì½ìŠµë‹ˆë‹¤.")
        args.audio_path = args.video_path
    # =================================================================

    if args.modality == "video":
        if not args.video_path: raise ValueError("--video_path required")
        video = load_video(args.video_path)
        video = video_transform(video)
        if len(video.shape) == 3: video = video.unsqueeze(1)
        if hasattr(args, 'downsample_ratio_video') and args.downsample_ratio_video:
            video = video[: video.size(0) // args.downsample_ratio_video * args.downsample_ratio_video]
        batch_data["video"] = video
        batch_data["tokens"] = ""

    else:
        # ì˜¤ë””ì˜¤ ëª¨ë‹¬ë¦¬í‹°ê°€ í¬í•¨ëœ ê²½ìš° (audio, audiovisual)
        if not args.audio_path: 
            raise ValueError("--audio_path (or --video_path for audiovisual) required")

        # 1. ì˜¤ë””ì˜¤ ë¡œë“œ (ë¹„ë””ì˜¤ íŒŒì¼ì„ ë„£ì–´ë„ torchaudioê°€ ì•Œì•„ì„œ ì†Œë¦¬ë§Œ ë½‘ì•„ì˜´)
        waveform, sample_rate = torchaudio.load(args.audio_path, normalize=True)
        
        # 2. 16k ë¦¬ìƒ˜í”Œë§
        if sample_rate != 16000: 
            waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)
        
        # 3. Mono ë³€í™˜
        if waveform.shape[0] > 1: 
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        # 4. Transpose [Time, 1]
        audio = waveform.transpose(1, 0)
        
        # 5. ë¹„ë””ì˜¤ ì²˜ë¦¬ (Audiovisualì¼ ë•Œ)
        if args.modality in ["audiovisual", "audiovisual_avhubert"]:
            if not args.video_path: raise ValueError("--video_path required for audiovisual")
            
            video = load_video(args.video_path)
            audio = cut_or_pad(audio, len(video) * rate_ratio) # ê¸¸ì´ ë§ì¶”ê¸°
            video = video_transform(video)
            
            # ë¹„ë””ì˜¤ ì±„ë„ ì°¨ì› ì¶”ê°€ [T, H, W] -> [T, 1, H, W]
            if len(video.shape) == 3: video = video.unsqueeze(1)
            
            if hasattr(args, 'downsample_ratio_video') and args.downsample_ratio_video:
                if args.modality == "audiovisual_avhubert" and hasattr(args, 'single_projector_avhubert') and args.single_projector_avhubert:
                    pass
                else:
                    video = video[: video.size(0) // args.downsample_ratio_video * args.downsample_ratio_video]
            batch_data["video"] = video

        audio = audio_transform(audio)
        batch_data["audio"] = audio
        batch_data["tokens"] = ""

    # ë°°ì¹˜ ìƒì„±
    batch_list = [batch_data]
    batch = collate_LLM(batch_list, modelmodule.tokenizer, args.modality, is_trainval=False)
    
    # ì°¨ì› ìˆ˜ë™ êµì • (Squeeze)
    if "audio" in batch:
        audio_tensor = batch["audio"]
        if audio_tensor.dim() == 4 and audio_tensor.shape[1] == 1:
            batch["audio"] = audio_tensor.squeeze(1)
    if "video" in batch:
        video_tensor = batch["video"]
        if video_tensor.dim() == 6 and video_tensor.shape[1] == 1:
            batch["video"] = video_tensor.squeeze(1)
    
    # GPU ì´ë™ ë° íƒ€ì… ë³€í™˜
    device = next(modelmodule.model.parameters()).device
    target_dtype = torch.bfloat16 if next(modelmodule.model.parameters()).dtype == torch.bfloat16 else torch.float32

    for key in batch:
        if isinstance(batch[key], torch.Tensor):
            tensor = batch[key]
            if tensor.is_floating_point():
                batch[key] = tensor.to(device=device, dtype=target_dtype)
            else:
                batch[key] = tensor.to(device=device)
    
    # ì¶”ë¡  ì‹¤í–‰
    modelmodule.eval()
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    
    with torch.inference_mode():
        generated_ids = modelmodule.model(batch, is_trainval=False)
        generated_text = modelmodule.tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]
    
    if torch.cuda.is_available(): torch.cuda.empty_cache()
    print_memory_usage("After Inference")
    
    return generated_text

def main():
    args = parse_args()
    modelmodule = load_model_from_checkpoint(args.checkpoint, args)
    predicted_text = inference_single_file(args, modelmodule)
    
    print(f"\n{'='*50}")
    print(f"ìƒì„±ëœ í…ìŠ¤íŠ¸: {predicted_text}")
    print(f"{'='*50}\n")
    
    with open(args.output_txt, "w", encoding="utf-8") as f:
        f.write(predicted_text + "\n")
    print(f"Inference result saved to {args.output_txt}")

if __name__ == "__main__":
    main()